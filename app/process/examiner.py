import logging

from app.config import InferenceConfig
from app.control.post.examiner import post_process
from app.exceptions.exception import InferenceFailure, LogicError
from app.llm.base import LLMBaseModel
from app.llm.model import LLM, LLMType
from app.models.content import Task
from app.prompts.config import PromptMessageConfig
from app.prompts.examiner.anthropic import (
    generate_anthropic_examiner_system_message,
    generate_anthropic_examiner_user_message)
from app.prompts.examiner.cohere import (
    generate_cohere_examiner_system_message,
    generate_cohere_examiner_user_message)
from app.prompts.examiner.google_ai import (
    generate_google_ai_examiner_system_message,
    generate_google_ai_examiner_user_message)
from app.prompts.examiner.llama3 import generate_llama3_examiner_system_message, generate_llama3_examiner_user_message
from app.prompts.examiner.open_ai import (
    generate_open_ai_examiner_system_message,
    generate_open_ai_examiner_user_message)

log = logging.getLogger(__name__)

class Examiner:
    TASK = Task.PRACTICE
    
    _llm_type: LLMType
    _model: LLMBaseModel

    def __init__(self, config: InferenceConfig):
        self._llm_type = config.llm_type.get(self.TASK)
        self._model = LLM(model_type=self._llm_type).model

    def generate_system_message(self) -> str:
        match self._llm_type:
            case LLMType.OPENAI_GPT4:
                return generate_open_ai_examiner_system_message()
            case LLMType.OPENAI_GPT3_5:
                return generate_open_ai_examiner_system_message()
            case LLMType.GEMINI_PRO:
                return generate_google_ai_examiner_system_message()
            case LLMType.CLAUDE_3_SONNET:
                return generate_anthropic_examiner_system_message()
            case LLMType.CLAUDE_INSTANT_1:
                return generate_anthropic_examiner_system_message()
            case LLMType.COHERE_COMMAND_R:
                return generate_cohere_examiner_system_message()
            case LLMType.COHERE_COMMAND_R_PLUS:
                return generate_cohere_examiner_system_message()
            case LLMType.LLAMA3:
                return generate_llama3_examiner_system_message()

    def generate_user_message(self, topic: str, summary_chunk: str) -> str:
        match self._llm_type:
            case LLMType.OPENAI_GPT4:
                return generate_open_ai_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.OPENAI_GPT3_5:
                return generate_open_ai_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.GEMINI_PRO:
                return generate_google_ai_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.CLAUDE_3_SONNET:
                return generate_anthropic_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.CLAUDE_INSTANT_1:
                return generate_anthropic_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.COHERE_COMMAND_R:
                return generate_cohere_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.COHERE_COMMAND_R_PLUS:
                return generate_cohere_examiner_user_message(
                    topic=topic, summary_chunk=summary_chunk
                )
            case LLMType.LLAMA3:
                return generate_llama3_examiner_user_message()

    async def examine(self, topic: str, summary_chunk: str) -> tuple[str, str, str, str]:
        """This method generates a practice question and answer for a given topic and summary chunk.

        Args:
            topic (str): The topic of the summary chunk.
            summary_chunk (str): The summary chunk that will give the LLM the context to generate the question and answer.

        Returns:
            tuple[str, str, str, str]: The language, question, half-completed code and fully-completed code of the practice generated by the LLM.
        """
        system_message: str = self.generate_system_message()
        user_message: str = self.generate_user_message(
            topic=topic, summary_chunk=summary_chunk
        )

        try:
            language, question, half_completed_code, fully_completed_code = await self._model.send_message(
                system_message=system_message, user_message=user_message, config=PromptMessageConfig.PRACTICE
            )
            language, question, half_completed_code, fully_completed_code = post_process(
                language=language, question=question, half_completed_code=half_completed_code, fully_completed_code=fully_completed_code
            )
            return language, question, half_completed_code, fully_completed_code
        except LogicError as e:
            log.error(f"Logic error occurred while generating practices off the summary: {e}")
            raise e
        except InferenceFailure as e:
            log.error(f"Inference failure occurred while generating practices off the summary: {e}")
            raise e
        except Exception as e:
            log.error(f"Unexpected error occurred while generating practices off the summary: {e}")
            raise e
