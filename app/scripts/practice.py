import logging
import asyncio
from typing import Any
from app.config import InferenceConfig
from app.exceptions.exception import InferenceFailure, LogicError
from app.process.examiner import Examiner
from app.prompts.summariser.functions import SummaryFunctions

log = logging.getLogger(__name__)


async def generate_practice(
    summary: list[dict[str, Any]]
) -> list[dict[str, str]]:
    
    tasks = [
        _generate(summary_chunk) for summary_chunk in summary
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    practice: list[dict[str, str]] = []
    failures: int = 0
    for (topic, summary_chunk), result in zip(summary.items(), results):
        if isinstance(result, Exception):
            # TODO: Handle InferenceFailure and LogicError separately if necessary (but we should still pokemon catch Exception here to ensure we don't accidentally append None/unexpected stuff to practice)
            log.error(f"Error processing practice for topic: {topic} - {result}")
            failures += 1
            continue
        practice.append(
            {
                "language": result[0],
                "question": result[1],
                "half_completed_code": result[2],
                "fully_completed_code": result[3],
            }
        )
    if failures:
        log.warning(f"{failures} out of {len(summary)} tasks failed during practice generation.")
    
    return practice
        
async def _generate(
    summary_chunk: dict[str, Any], 
    attempt=1, 
    max_attempts=1
) -> tuple[str, str, str]:
    """Generates a practice question and answer for a given topic and summary chunk.

    Args:
        topic (str): The topic of the summary chunk
        summary_chunk (str): The summary chunk that will give the LLM the context 
        attempt (int, optional): The current attempt number after the output is rejected previously. Defaults to 1.
        max_attempts (int, optional): The max number of attempts before we stop retrying. Defaults to 9.
        
    Returns:
        tuple[str, str, str]: The language, question, and answer of the practice generated by the LLM.
    """
    config = InferenceConfig()
    examiner = Examiner(config=config)
    try:
        language, question, half_completed_code, fully_completed_code = await examiner.examine(
            topic=summary_chunk[SummaryFunctions.TOPIC.value],
             
            summary_chunk=summary_chunk
        )
        return language, question, half_completed_code, fully_completed_code
    except LogicError as e:
        log.error(
            f"Logic error occurred while generating practice (attempt {attempt}/{max_attempts}): {e}"
        )
    except InferenceFailure as e:
        log.error(
            f"Inference failure occurred while generating practice (attempt {attempt}/{max_attempts}): {e}"
        )

    if attempt < max_attempts:
        log.info(f"Retrying practice generation for topic: {topic}...")
        return await _generate(
            topic=topic,
            summary_chunk=summary_chunk,
            attempt=attempt + 1,
            max_attempts=max_attempts,
        )
    else:
        raise InferenceFailure(
            f"Failed to post-process practice for topic: {topic} after {max_attempts} attempts."
        )
